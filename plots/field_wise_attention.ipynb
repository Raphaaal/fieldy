{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "866a190b-20c5-4429-b6de-a2eda1a9f4d2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757e958-0e4e-4045-a388-21b4171ec9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f49f-9daf-4a3f-87b7-5c1f7c2fdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoConfig, \n",
    "    utils,\n",
    ")\n",
    "from bertviz import head_view\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from main import load_prsa\n",
    "\n",
    "from models.configs import (\n",
    "    RowTabBERTConfig,\n",
    "    FieldyConfig,\n",
    ")\n",
    "\n",
    "from models.models import (\n",
    "    Model,\n",
    "    RowTabBERT,\n",
    "    Fieldy,\n",
    ")\n",
    "\n",
    "from dataset.datacollator import (\n",
    "    RowTabBERTDataCollatorForLanguageModeling,\n",
    "    RowTabBERTDataCollatorForFineTuning,\n",
    "    FieldyDataCollatorForLanguageModeling,\n",
    "    FieldyDataCollatorForFineTuning,\n",
    ")\n",
    "\n",
    "from args import define_main_parser\n",
    "\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa96e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040e591-fab3-4e75-8103-4e6f7b3d31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_suffix = \"\"\n",
    "data_type = \"prsa\"\n",
    "\n",
    "parser = define_main_parser()\n",
    "args_row_tabbert = parser.parse_args(args=[\n",
    "    f\"--data-type=prsa\",\n",
    "    \"--family=row_tabbert\",\n",
    "    \"--hidden-size=800\",\n",
    "    \"--fieldtransf-nheads=10\",\n",
    "    \"--fieldtransf-nlayers=6\",\n",
    "    \"--n-heads=10\",\n",
    "    \"--n-layers=10\",\n",
    "    \"--scale-targets\",\n",
    "    \"--scaling=std\",\n",
    "    \"--dropout=0.1\",\n",
    "    \"--pos-emb\",\n",
    "    \"--col-emb\",\n",
    "    \"--pt-epochs=24\",\n",
    "    \"--ft-epochs=20\",\n",
    "    \"--seed=1\",\n",
    "])\n",
    "args_fieldy = parser.parse_args(args=[\n",
    "    f\"--data-type=prsa\",\n",
    "    \"--hidden-size=800\",\n",
    "    \"--fieldtransf-nheads=10\",\n",
    "    \"--fieldtransf-nlayers=8\",\n",
    "    \"--n-heads=10\",\n",
    "    \"--n-layers=4\",\n",
    "    \"--scale-targets\",\n",
    "    \"--scaling=std\",\n",
    "    \"--dropout=0.1\",\n",
    "    \"--pos-emb\",\n",
    "    \"--col-emb\",\n",
    "    \"--pt-epochs=24\",\n",
    "    \"--ft-epochs=20\",\n",
    "    \"--seed=1\",\n",
    "])\n",
    "\n",
    "pt_ep = 24\n",
    "ft_ep = 20\n",
    "posemb = \"posemb\"\n",
    "colemb = \"colemb\"\n",
    "num_ft_labels = 10 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d56477-f71d-4aa5-9e62-792cfc3f5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires to have proprocessed the PRSA dataset (execute prsa.sh if needed)\n",
    "with open(f\"./data/prsa/PRSADataset_labeled{name_suffix}.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9484e68-f90b-446d-9bbe-c16687730ceb",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Requires to have trained the models on the PRSA dataset (execute `prsa.sh` if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7025298",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_tabbert = Model(\n",
    "    special_tokens=dataset.vocab.get_special_tokens(),\n",
    "    vocab=dataset.vocab,\n",
    "    family=args_row_tabbert.family,\n",
    "    ncols=dataset.ncols,\n",
    "    hidden_size=args_row_tabbert.hidden_size,\n",
    "    seq_len=dataset.seq_len,\n",
    "    pos_emb=args_row_tabbert.pos_emb,\n",
    "    col_emb=args_row_tabbert.col_emb,\n",
    "    max_position_embeddings=512,\n",
    "    mlm_loss=args_row_tabbert.mlm_loss,\n",
    "    n_heads=args_row_tabbert.n_heads,\n",
    "    fieldtransf_nheads=args_row_tabbert.fieldtransf_nheads,\n",
    "    fieldtransf_nlayers=args_row_tabbert.fieldtransf_nlayers,\n",
    "    n_layers=args_row_tabbert.n_layers,\n",
    "    num_ft_labels=num_ft_labels,\n",
    "    dropout=args_row_tabbert.dropout,\n",
    ")\n",
    "AutoConfig.register(\"RowTabBERT\", RowTabBERTConfig)\n",
    "AutoModelForSequenceClassification.register(RowTabBERTConfig, RowTabBERT)\n",
    "row_tabbert_model_path_pt = f\"../results/{args_row_tabbert.data_type}/RowTabBERT/RowTabBERT_{args_row_tabbert.fieldtransf_nheads}fieldtransfheads_{args_row_tabbert.fieldtransf_nlayers}fieldtransflayers_{args_row_tabbert.hidden_size}hs_{args_row_tabbert.n_heads}heads_{args_row_tabbert.n_layers}layers_{'posemb' if args_row_tabbert.pos_emb else 'noposemb'}_{'colemb' if args_row_tabbert.col_emb else 'nocolemb'}_MSE_pt{args_row_tabbert.pt_epochs}ep_ft{args_row_tabbert.ft_epochs}ep_seed{args_row_tabbert.seed}/pt\" \n",
    "row_tabbert_model_pt = AutoModel.from_pretrained(row_tabbert_model_path_pt, vocab=dataset.vocab, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91347ffa-ddcd-4e2c-8b15-5ea2a8d25d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldy = Model(\n",
    "    special_tokens=dataset.vocab.get_special_tokens(),\n",
    "    vocab=dataset.vocab,\n",
    "    family=args_fieldy.family,\n",
    "    ncols=dataset.ncols,\n",
    "    hidden_size=args_fieldy.hidden_size,\n",
    "    seq_len=dataset.seq_len,\n",
    "    pos_emb=args_fieldy.pos_emb,\n",
    "    col_emb=args_fieldy.col_emb,\n",
    "    max_position_embeddings=512,\n",
    "    mlm_loss=args_fieldy.mlm_loss,\n",
    "    n_heads=args_fieldy.n_heads,\n",
    "    fieldtransf_nheads=args_fieldy.fieldtransf_nheads,\n",
    "    fieldtransf_nlayers=args_fieldy.fieldtransf_nlayers,\n",
    "    n_layers=args_fieldy.n_layers,\n",
    "    num_ft_labels=num_ft_labels,\n",
    "    dropout=args_fieldy.dropout,\n",
    ")\n",
    "AutoConfig.register(\"Fieldy\", FieldyConfig)\n",
    "AutoModelForSequenceClassification.register(FieldyConfig, Fieldy)\n",
    "fieldy_model_path_pt = f\"../results/{args_fieldy.data_type}/Fieldy/Fieldy_{args_fieldy.fieldtransf_nheads}fieldtransfheads_{args_fieldy.fieldtransf_nlayers}fieldtransflayers_{args_fieldy.hidden_size}hs_{args_fieldy.n_heads}heads_{args_fieldy.n_layers}layers_{'posemb' if args_fieldy.pos_emb else 'noposemb'}_{'colemb' if args_fieldy.col_emb else 'nocolemb'}_MSE_pt{args_fieldy.pt_epochs}ep_ft{args_fieldy.ft_epochs}ep_seed{args_fieldy.seed}/pt\" \n",
    "fieldy_model_pt = AutoModel.from_pretrained(fieldy_model_path_pt, vocab=dataset.vocab, output_attentions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc59d668-80f0-47c0-a4ac-f4748b638e1c",
   "metadata": {},
   "source": [
    "# Create samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814dce5-0553-40e5-bfc3-730c3b1c80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabbert_data_collator_pt = RowTabBERTDataCollatorForLanguageModeling(\n",
    "    tokenizer=row_tabbert.tokenizer, \n",
    "    mlm=args_row_tabbert.mlm, \n",
    "    mlm_probability=args_row_tabbert.mlm_prob,\n",
    "    ncols=dataset.ncols,\n",
    "    seq_len=dataset.seq_len,\n",
    "    data_type=args_row_tabbert.data_type,\n",
    "    seed=args_row_tabbert.seed,\n",
    "    randomize_seq=True;\n",
    ")\n",
    "\n",
    "fieldy_data_collator_pt = FieldyDataCollatorForLanguageModeling(\n",
    "    tokenizer=fieldy.tokenizer, \n",
    "    mlm=args_fieldy.mlm, \n",
    "    mlm_probability=args_fieldy.mlm_prob,\n",
    "    ncols=dataset.ncols,\n",
    "    seq_len=dataset.seq_len,\n",
    "    data_type=args_fieldy.data_type,\n",
    "    seed=args_fieldy.seed,\n",
    "    randomize_seq=True;\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a82814",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "\n",
    "samples = []\n",
    "for i in range(0, nsamples):\n",
    "    samples.append(dataset.__getitem__(i))\n",
    "\n",
    "rows_of_unk = [] \n",
    "rows_of_mask = [5,6,7,8,9] \n",
    "except_row = 0\n",
    "fields_of_unk = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] \n",
    "fields_of_mask = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f61bdd-4d4e-4a9b-82fd-d323404dc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pt_row_tabbert = fieldy_data_collator_pt(samples)\n",
    "for i in range(0, nsamples): \n",
    "    for row in rows_of_unk:\n",
    "        for field in fields_of_unk:\n",
    "            samples_pt_row_tabbert['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[UNK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "    for row in rows_of_mask:\n",
    "        if row == except_row:\n",
    "            for field in range(16):\n",
    "                samples_pt_row_tabbert['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[MASK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "        else:\n",
    "            for field in fields_of_mask:\n",
    "                samples_pt_row_tabbert['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[MASK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "samples_pt_row_tabbert['attention_mask'] = None \n",
    "samples_pt_row_tabbert['masked_lm_labels'] = samples_pt_row_tabbert['labels']\n",
    "for k, v in samples_pt_row_tabbert.items():\n",
    "    if torch.is_tensor(v):\n",
    "        samples_pt_row_tabbert[k] = v.to(device)\n",
    "row_tabbert_model_pt.to(device)\n",
    "\n",
    "samples_pt_fieldy = fieldy_data_collator_pt(samples)\n",
    "for i in range(0, nsamples): \n",
    "    for row in rows_of_unk:\n",
    "        for field in fields_of_unk:\n",
    "            samples_pt_fieldy['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[UNK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "    for row in rows_of_mask:\n",
    "        if row == except_row:\n",
    "            for field in range(16):\n",
    "                samples_pt_fieldy['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[MASK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "        else:\n",
    "            for field in fields_of_mask:\n",
    "                samples_pt_fieldy['input_ids'][i][row][field] = dataset.vocab.token2id[\"SPECIAL\"][\"[MASK]\"][0] # We [MASK] 1 token in each sample(Vocab ID 4)\n",
    "samples_pt_fieldy['attention_mask'] = None \n",
    "samples_pt_fieldy['masked_lm_labels'] = samples_pt_fieldy['labels']\n",
    "for k, v in samples_pt_fieldy.items():\n",
    "    if torch.is_tensor(v):\n",
    "        samples_pt_fieldy[k] = v.to(device)\n",
    "fieldy_model_pt.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459d653d-2475-4f0d-9972-ed3765d386d5",
   "metadata": {},
   "source": [
    "# Field-wise attention toy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6be03-f20e-43b3-970b-ba14a0a24f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(samples_pt, model_pt, family=\"row_tabbert\"):\n",
    "    \n",
    "    samples_for_scoring = {}\n",
    "    for k, v in samples_pt.items():\n",
    "        try:\n",
    "            # samples_for_scoring[k] = v[sample_id].unsqueeze(0)\n",
    "            samples_for_scoring[k] = v\n",
    "        except TypeError: # \"None\", e.g. for attention_mask\n",
    "            samples_for_scoring[k] = v\n",
    "    preds, full_outputs, preds_scores = model_pt(**samples_for_scoring, output_attentions=True)\n",
    "\n",
    "    gt = samples_pt[\"input_ids\"][:, :, 14][:, :5]\n",
    "    gt = gt.cpu().numpy().tolist()\n",
    "    gt_hours = []\n",
    "    for s in gt:\n",
    "        gt_hours.append([dataset.vocab.id2token[i][0] for i in s])\n",
    "    gt_hours = [\n",
    "        [(s+5)%24 for s in h] \n",
    "        for h in gt_hours\n",
    "    ]\n",
    "    gt_hours = np.array(gt_hours).flatten()\n",
    "\n",
    "    guesses = []\n",
    "    if family == \"fieldy\":\n",
    "        for i, s in enumerate(preds_scores):\n",
    "            s_guesses = []\n",
    "            for h in range(5, 10):\n",
    "                mlm_guess = torch.topk(preds_scores[i, (10*14) + h, :], 1).indices # Tabbie has been flattened from [bs, ncols, nrows]\n",
    "                token_guess = [dataset.vocab.id2token[top.item()][:2] for top in mlm_guess][0]\n",
    "                if token_guess[1] == \"hour\":\n",
    "                    s_guesses.append(token_guess[0])\n",
    "                else:\n",
    "                    s_guesses.append(-1)\n",
    "            guesses.append(s_guesses)\n",
    "    elif family == \"row_tabbert\":\n",
    "        for i, s in enumerate(preds_scores):\n",
    "            s_guesses = []\n",
    "            for h in range(5, 10):\n",
    "                mlm_guess = torch.topk(preds_scores[i, 14 + (16*h), :], 1).indices # Tabbie has been flattened from [bs, ncols, nrows]\n",
    "                token_guess = [dataset.vocab.id2token[top.item()][:2] for top in mlm_guess][0]\n",
    "                if token_guess[1] == \"hour\":\n",
    "                    s_guesses.append(token_guess[0])\n",
    "                else:\n",
    "                    s_guesses.append(-1)\n",
    "            guesses.append(s_guesses)\n",
    "    guesses = np.array(guesses).flatten()\n",
    "\n",
    "    score = (gt_hours == guesses).sum()\n",
    "    score = score / guesses.shape[0]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(samples_pt_row_tabbert, row_tabbert_model_pt, family=\"row_tabbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a00145-65da-44dd-a0ac-d2cb9ca89bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(samples_pt_fieldy, fieldy_model_pt, family=\"fieldy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
