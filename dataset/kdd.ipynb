{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from copy import deepcopy\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataset.kdd import KDDDataset\n",
    "from dataset.vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = None\n",
    "timedelta_suffix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"TabBERT\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_representation = \"as int\"\n",
    "quantize_num_cols = True\n",
    "n_bins = 50\n",
    "add_step_sep_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path_pretrain = '../data/kdd/pkdd99_pretraining_post-encoding.csv'\n",
    "load_path_finetune = '../data/kdd/pkdd99_finetuning_post-encoding.csv'\n",
    "\n",
    "seq_len = 150 # NB: this is `t_max` in the UniTTab paper.\n",
    "min_frequency = 0.\n",
    "grouping_col = \"account_id\"\n",
    "ts_col = \"timestamp\"\n",
    "ordered_cols = [\"amount_trans\", \"balance\", \"k_symbol\", \"operation\", \"type_trans\", \"Year\", \"Month\", \"Day\", \"weekday\"]\n",
    "init_categorical_indicator = [False, False, True, True, True, True, True, True, True]\n",
    "delta_features = [\n",
    "    {\n",
    "        \"name\": \"delta_days\",\n",
    "        \"unit\": \"timedelta64[D]\",\n",
    "    },\n",
    "]\n",
    "label_col = \"status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_pt = pd.read_csv(load_path_pretrain, nrows=nrows)\n",
    "raw_df_pt[\"pretraining\"] = 1\n",
    "print(raw_df_pt.shape)\n",
    "df_pt = raw_df_pt.copy(deep=True)\n",
    "\n",
    "raw_df_ft = pd.read_csv(load_path_finetune, nrows=nrows)\n",
    "raw_df_ft[\"pretraining\"] = 0\n",
    "print(raw_df_ft.shape)\n",
    "df_ft = raw_df_ft.copy(deep=True)\n",
    "\n",
    "raw_df = pd.concat([raw_df_pt, raw_df_ft])\n",
    "print(raw_df.shape)\n",
    "df = raw_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle timestamp representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ts_col] = \"19\" + df[\"Year\"].astype(str) + \"-\" + df[\"Month\"].astype(str) + \"-\" + df[\"Day\"].astype(str)\n",
    "df[ts_col] = pd.to_datetime(df[ts_col], format=\"%Y-%m-%d\")\n",
    "df[\"date\"] = df[ts_col]\n",
    "df['weekday'] = df['date'].dt.dayofweek\n",
    "df = df.drop(columns=[\"date\"])\n",
    "\n",
    "if \"as int\" == ts_representation:\n",
    "    df[f\"{ts_col}_int\"] = df[ts_col].astype(int)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df[f\"{ts_col}_int\"] = min_max_scaler.fit_transform(df[f\"{ts_col}_int\"].to_numpy().reshape(-1, 1))\n",
    "    ordered_cols += [f\"{ts_col}_int\"]\n",
    "    init_categorical_indicator += [False]\n",
    "\n",
    "if \"as int and delta\" == ts_representation:\n",
    "    timedelta_suffix = \"_timedelta\"\n",
    "    \n",
    "    df[f\"{ts_col}_int\"] = df[ts_col].astype(int)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df[f\"{ts_col}_int\"] = min_max_scaler.fit_transform(df[f\"{ts_col}_int\"].to_numpy().reshape(-1, 1))\n",
    "    df = df.drop(columns=[\"Year\", \"Month\", \"Day\"])\n",
    "\n",
    "    ordered_cols += [f\"{ts_col}_int\"]\n",
    "    init_categorical_indicator += [False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantize_num_cols:\n",
    "    for i, (c, cat) in enumerate(zip(ordered_cols, init_categorical_indicator)):\n",
    "        if not cat and c not in [ts_col, \"timedelta\"]:\n",
    "            print(c)\n",
    "            quantizer = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"quantile\", subsample=None)\n",
    "            df[c] = quantizer.fit_transform(df[c].to_numpy().reshape(-1, 1))\n",
    "            init_categorical_indicator[i] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if min_frequency > 0:\n",
    "    for col, cat in zip(ordered_cols, init_categorical_indicator):\n",
    "        if cat and col not in [\"timedelta\"]:\n",
    "            print(col)\n",
    "            series = df[col].value_counts()\n",
    "            series_pct = (series / series.sum())\n",
    "            infrequent_mask = series_pct < min_frequency\n",
    "            # Replace infrequent categories by -1 \n",
    "            df[col] = np.where(\n",
    "                df[col].isin(series[infrequent_mask].index), \n",
    "                -1,\n",
    "                df[col]\n",
    "            )\n",
    "            print(f\"Nb of affected rows: {df[df[col] == -1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories ordinal encoding from 1 to n_category.\n",
    "It was already done (but starting at 0 instead of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPECIAL_TOKENS = 7\n",
    "vocab_start = 0\n",
    "for col, cat in zip(ordered_cols, init_categorical_indicator):\n",
    "    if cat:\n",
    "        print(col)\n",
    "        ordinal_enc = OrdinalEncoder()\n",
    "        col_values = df[col].to_numpy().reshape(-1, 1).astype(str)\n",
    "        df[col] = ordinal_enc.fit_transform(col_values)\n",
    "        df[col] = df[col] + N_SPECIAL_TOKENS + vocab_start # Common vocab\n",
    "        vocab_start = vocab_start + len(ordinal_enc.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the average number of transactions per client\n",
    "df.groupby(\"account_id\").count()[\"Day\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the max number of transactions per client\n",
    "max_len = df.groupby(\"account_id\").count()[\"Day\"].max()\n",
    "max_len\n",
    "\n",
    "# Take the maximum length from now on\n",
    "seq_len = max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Make sequential dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary() \n",
    "\n",
    "file_name = '../data/kdd/vocab.nb'\n",
    "vocab.filename = file_name\n",
    "\n",
    "if \"delta\" in ts_representation:\n",
    "    vocab.timedelta_colid = 6 # Last ordered col will be timedelta\n",
    "\n",
    "vocab.set_field_keys([c for c in ordered_cols if c != \"timedelta\"])\n",
    "all_vocab = []\n",
    "for col in ordered_cols:\n",
    "    if col != \"timedelta\":\n",
    "        tokens = df[col].drop_duplicates().tolist()\n",
    "        for t in tokens:\n",
    "            vocab.set_id(t, col, return_local=False)\n",
    "\n",
    "vocab.save_vocab(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df_pt = df[df[\"pretraining\"] == 1]\n",
    "seq_df_pt = seq_df_pt.sort_values(ts_col).groupby(grouping_col).head(seq_len)\n",
    "if \"delta\" in ts_representation:\n",
    "    seq_df_pt = seq_df_pt[ordered_cols + [ts_col, grouping_col]]\n",
    "else:\n",
    "    seq_df_pt = seq_df_pt[ordered_cols + [grouping_col]]\n",
    "\n",
    "dataset_pt = []\n",
    "labels_pt = []\n",
    "for idx, group in seq_df_pt.groupby(grouping_col):\n",
    "\n",
    "    if \"delta\" in ts_representation:\n",
    "        group[\"timedelta\"] = group[ts_col]\n",
    "        ts_init = group[0:1][ts_col].values[0]\n",
    "        group[\"timedelta\"] = (group[\"timedelta\"] - ts_init).dt.days\n",
    "        dataset_pt.append(group[ordered_cols + [\"timedelta\"]].values)\n",
    "        ncols = group[ordered_cols + [\"timedelta\"]].shape[1]\n",
    "\n",
    "    else:\n",
    "        dataset_pt.append(group[ordered_cols].values)\n",
    "        ncols = group[ordered_cols].shape[1]\n",
    "        \n",
    "    labels_pt.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df_pt.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_pt = KDDDataset(samples=dataset_pt, targets=labels_pt, vocab=vocab, ncols=ncols, seq_len=seq_len, data=seq_df_pt, data_root=load_path_pretrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning dataset (all transactions -> Last tmax handled by data collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df_ft = df[df[\"pretraining\"] == 0]\n",
    "seq_df_ft = seq_df_ft.sort_values(ts_col).groupby(grouping_col).head(seq_len)\n",
    "if \"delta\" in ts_representation:\n",
    "    seq_df_ft = seq_df_ft[ordered_cols + [ts_col, grouping_col, label_col]]\n",
    "else:\n",
    "    seq_df_ft = seq_df_ft[ordered_cols + [grouping_col, label_col]]\n",
    "\n",
    "dataset_ft = []\n",
    "labels_ft = []\n",
    "for idx, group in seq_df_ft.groupby(grouping_col):\n",
    "\n",
    "    if \"delta\" in ts_representation:\n",
    "        group[\"timedelta\"] = group[ts_col]\n",
    "        ts_init = group[0:1][ts_col].values[0]\n",
    "        group[\"timedelta\"] = (group[\"timedelta\"] - ts_init).dt.days\n",
    "        dataset_ft.append(group[ordered_cols + [\"timedelta\"]].values)\n",
    "        ncols = group[ordered_cols + [\"timedelta\"]].shape[1]\n",
    "\n",
    "    else:\n",
    "        dataset_ft.append(group[ordered_cols].values)\n",
    "        ncols = group[ordered_cols].shape[1]\n",
    "        \n",
    "    labels_ft.append(group[label_col].values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df_ft.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_ft = KDDDataset(samples=dataset_ft, targets=labels_ft, vocab=vocab, ncols=ncols, seq_len=seq_len, data=seq_df_ft, data_root=load_path_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([l for l in labels_ft if l == 1]) / len(labels_ft) # ~11%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning dataset (tmax random transactions) -> Increase dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 10\n",
    "stride = tmax\n",
    "\n",
    "seq_df_ft_rand = df[df[\"pretraining\"] == 0]\n",
    "seq_df_ft_rand = seq_df_ft_rand.sort_values(ts_col).groupby(grouping_col).head(seq_len)\n",
    "if \"delta\" in ts_representation:\n",
    "    seq_df_ft_rand = seq_df_ft_rand[ordered_cols + [ts_col, grouping_col, label_col]]\n",
    "else:\n",
    "    seq_df_ft_rand = seq_df_ft_rand[ordered_cols + [grouping_col, label_col]]\n",
    "\n",
    "dataset_ft_rand = []\n",
    "labels_ft_rand = []\n",
    "for idx, group in seq_df_ft_rand.groupby(grouping_col):\n",
    "    \n",
    "    if group[ordered_cols].shape[0] < tmax:\n",
    "        dataset_ft_rand.append(group[ordered_cols].values[-tmax:])\n",
    "        labels_ft_rand.append(group[label_col].values[-1])\n",
    "    else:\n",
    "        max_len = group[ordered_cols].shape[0]\n",
    "        ncols = group[ordered_cols].shape[1]\n",
    "        n_examples = int(group[ordered_cols].shape[0] / tmax)\n",
    "        for i in range(n_examples):\n",
    "            dataset_ft_rand.append(group[ordered_cols][max_len-(i+1)*stride: max_len-i*stride].values)\n",
    "            labels_ft_rand.append(group[label_col].values[-1])\n",
    "assert len(labels_ft_rand) == len(dataset_ft_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_ft_rand = KDDDataset(samples=dataset_ft_rand, targets=labels_ft_rand, vocab=vocab, ncols=ncols, seq_len=seq_len, data=seq_df_ft, data_root=load_path_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([l for l in labels_ft_rand if l == 1]) / len(labels_ft_rand) # ~10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_ft_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_pt = f\"../data/kdd/KDDDataset_pt.pkl\"\n",
    "\n",
    "with open(save_path_pt, \"wb\") as f:\n",
    "    pickle.dump(kdd_pt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_ft = f\"../data/kdd/KDDDataset_ft.pkl\"\n",
    "\n",
    "with open(save_path_ft, \"wb\") as f:\n",
    "    pickle.dump(kdd_ft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_ft = f\"../data/kdd/KDDDataset_ft.pkl\"\n",
    "\n",
    "with open(save_path_ft, \"wb\") as f:\n",
    "    pickle.dump(kdd_ft_rand, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
