{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import dataset\n",
    "import torch\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from  sklearn.manifold import TSNE\n",
    "from  sklearn.decomposition import PCA\n",
    "from  sklearn.preprocessing import KBinsDiscretizer\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from  sklearn.preprocessing import RobustScaler\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "from  sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.option_context('display.max_rows', None, 'display.max_columns', None,'display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"kdd\"\n",
    "name_suffix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"kdd\":\n",
    "    with open(f\"./kdd/KDDDataset_ft{name_suffix}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: Accounts order when the dataset samples were preprared\n",
    "accounts = []\n",
    "for group in dataset.data.groupby(\"account_id\"):\n",
    "    account_name = group[0]\n",
    "    accounts.append(account_name)\n",
    "print(accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: raw initial data\n",
    "raw_data = pd.read_csv(\"./\" + dataset.data_root[8:])\n",
    "raw_data[\"date_raw\"] = \"19\" + raw_data[\"Year\"].astype(str) + \"-\" + raw_data[\"Month\"].astype(str) + \"-\" + raw_data[\"Day\"].astype(str)\n",
    "raw_data[\"date_raw\"] = pd.to_datetime(raw_data[\"date_raw\"], format=\"%Y-%m-%d\")\n",
    "raw_data['weekday'] = raw_data['date_raw'].dt.dayofweek\n",
    "raw_data = raw_data.sort_values(by=[\"date_raw\"])\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_data = dataset.data # Do NOT sort again (keep original timestamp sort)\n",
    "preprocessed_data = []\n",
    "for group, data in preproc_data.groupby(\"account_id\"):\n",
    "    preprocessed_data.append(data)\n",
    "preprocessed_data = pd.concat(preprocessed_data)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dataset.samples\n",
    "print(len(samples))\n",
    "\n",
    "targets = dataset.targets\n",
    "print(len(targets))\n",
    "\n",
    "print(preprocessed_data[\"account_id\"].drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys = list(dataset.vocab.token2id.keys())\n",
    "for k in vocab_keys:\n",
    "    print(f\"\\n--{k}--\")\n",
    "    # pprint(dataset.vocab.token2id[k])\n",
    "    print(len(dataset.vocab.token2id[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vocab.token2id[\"SPECIAL\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"account_id\"].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 1\n",
    "stride = 150\n",
    "final_id = sample_id\n",
    "\n",
    "# If pre-training dataset\n",
    "raw_sample = raw_data[raw_data[\"account_id\"] == final_id + 1][:stride]\n",
    "preprocessed_sample = preprocessed_data[preprocessed_data[\"account_id\"] == final_id + 1][:stride]\n",
    "\n",
    "# If fine-tuning dataset, check the offset depending on the account_ids included in the dataset (cf. cell above)\n",
    "offset = 18\n",
    "raw_sample = raw_data[raw_data[\"account_id\"] == final_id + offset][:stride]\n",
    "preprocessed_sample = preprocessed_data[preprocessed_data[\"account_id\"] == final_id + offset][:stride]\n",
    "\n",
    "pytorch_sample = torch.tensor(samples[sample_id]).reshape(-1, dataset.ncols) # Not always the same seq_len\n",
    "pytorch_target = torch.tensor(targets[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Need padding\n",
    "# dataset.samples[0].shape\n",
    "# dataset.samples[758].shape\n",
    "\n",
    "# Other note:\n",
    "# The notebook is very slow. Is it because the pickled dataset is heavy?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore (fine tuning dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if finetuning dataset: check class imbalance\n",
    "print(raw_data[[\"account_id\", \"status\"]].drop_duplicates().groupby(\"status\").count())\n",
    "print(preprocessed_data[[\"account_id\", \"status\"]].drop_duplicates().groupby(\"status\").count())\n",
    "print(torch.unique(torch.tensor(dataset.targets), return_counts=True))\n",
    "print(76/(606+76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transactions and Evolution of fraud over the years\n",
    "\n",
    "raw_data_ym = raw_data.copy(deep=True)\n",
    "raw_data_ym[\"year_month\"] = raw_data_ym[\"Year\"].astype(str) + \"-\" + raw_data_ym[\"Month\"].astype(str)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    ax=ax1,\n",
    "    x='year_month', \n",
    "    data=raw_data_ym,\n",
    ")\n",
    "sns.lineplot(\n",
    "    ax=ax2,\n",
    "    x='year_month', \n",
    "    y='status', \n",
    "    data=raw_data_ym,\n",
    "    estimator=\"mean\", # Aggregate the frauds by avging them\n",
    ")\n",
    "\n",
    "ax1.tick_params(axis='x', labelrotation=70)\n",
    "ax2.tick_params(axis='x', labelrotation=70)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Take aways: \n",
    "# 2x more transactions in 1996 and 1997\n",
    "# Always more transactions in January\n",
    "# Fraud slowly decreases over time\n",
    "# No obvious seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of fraud by day of the month\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    x='Day', \n",
    "    y='status',\n",
    "    estimator=\"mean\", \n",
    "    data=raw_data,\n",
    ")\n",
    "ax.legend(loc='upper left', ncol=6)\n",
    "ax.tick_params(axis='x', labelrotation=70)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Take aways: \n",
    "# - day of the month matters\n",
    "# - more fraud in the second half of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of fraud by weekday\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    x='status', \n",
    "    hue='weekday',\n",
    "    data=raw_data,\n",
    "    estimator=\"mean\",\n",
    ")\n",
    "ax.legend(loc='upper left', ncol=7)\n",
    "ax.tick_params(axis='x', labelrotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Take aways: \n",
    "# - day of the week matters a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    ax=ax1,\n",
    "    data=raw_data[raw_data[\"status\"] == 0],\n",
    "    y=\"amount_trans\", \n",
    "    x='balance',\n",
    "    hue=\"status\",\n",
    "    alpha=0.25,\n",
    "    size=1,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    ax=ax2,\n",
    "    data=raw_data[raw_data[\"status\"] == 1],\n",
    "    y=\"amount_trans\", \n",
    "    x='balance',\n",
    "    alpha=0.25,\n",
    "    hue=\"status\",\n",
    "    size=1,\n",
    ")\n",
    "\n",
    "# Take away:\n",
    "# - Numerical features alone do not seem indicative of Fraud\n",
    "# - transaction amount correlates positively with account balance (not surprising)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variable data exploration\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(12, 4))\n",
    "sns.countplot(\n",
    "    ax=ax1,\n",
    "    x='k_symbol', \n",
    "    hue='status', \n",
    "    data=raw_data,\n",
    ")\n",
    "sns.countplot(\n",
    "    ax=ax2,\n",
    "    x='operation', \n",
    "    hue='status', \n",
    "    data=raw_data,\n",
    ")\n",
    "sns.countplot(\n",
    "    ax=ax3,\n",
    "    x='type_trans', \n",
    "    hue='status', \n",
    "    data=raw_data,\n",
    ")\n",
    "\n",
    "# Take-away:\n",
    "# More type_trans == 2 for Frauds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
